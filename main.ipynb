{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b59afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import opendssdirect as dss\n",
    "from SAC_MARL.maddpg import MADDPG\n",
    "from SAC_MARL.replay_buffer import ReplayBuffer\n",
    "from config import IEEE123_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52da33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dss.Command('clear')\n",
    "dss.Text.Command(f'Redirect \"./123Bus/IEEE123Master.dss\"')\n",
    "dss.Solution.Solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab4cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = IEEE123_config.get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e777a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shuffled_weights(seed=32, total_numbers=91):\n",
    "    \"\"\"\n",
    "    Generates a shuffled array of numbers including 0 to 10 and additional random numbers between 0 and 10.\n",
    "\n",
    "    Parameters:\n",
    "    - seed (int): The seed for random number generation.\n",
    "    - total_numbers (int): The total number of numbers to generate, including 0 to 10.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: A shuffled tensor of numbers.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Step 1: Include the numbers from 0 to 10\n",
    "    numbers_0_to_10 = torch.arange(0, 11)\n",
    "\n",
    "    # Step 2: Generate the remaining random numbers between 0 and 10\n",
    "    remaining_numbers = torch.randint(0, 11, (total_numbers - len(numbers_0_to_10),))\n",
    "\n",
    "    # Step 3: Combine the arrays\n",
    "    combined_numbers = torch.cat((numbers_0_to_10, remaining_numbers))\n",
    "\n",
    "    # Step 4: Shuffle the combined array\n",
    "    shuffled_numbers = combined_numbers[torch.randperm(combined_numbers.size(0))]\n",
    "\n",
    "    return shuffled_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98d2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "class World:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.weights = generate_shuffled_weights()\n",
    "\n",
    "        self.num_areas = 5\n",
    "        self.agent_1 = MADDPG(args, 0) \n",
    "        self.agent_2 = MADDPG(args, 1) \n",
    "        self.agent_3 = MADDPG(args, 2) \n",
    "        self.agent_4 = MADDPG(args, 3) \n",
    "        self.agent_5 = MADDPG(args, 4) \n",
    "\n",
    "        \n",
    "        self.agent_n = list([self.agent_1,self.agent_2,self.agent_3,self.agent_4,self.agent_5])\n",
    "        \n",
    "        self.P_gen = 2600\n",
    "        \n",
    "        self.regions_switches = [\n",
    "            [('1.2','2.2'),('1.3','3.3'),('8.2','12.2'),('8.1','9.1'),('13.3','34.3'),('18.1','19.1'),('21.2','22.2'),('23.3','24.3'),('25r.1.3', '26.1.3'),('25.1.2.3','28.1.2.3')],\n",
    "            [('35.1.2','36.1.2'),('40.3','41.3'),('42.2','43.2'),('44.1','45.1'),('44.1.2.3','47.1.2.3')],\n",
    "            [('57.2','58.2'),('60.1.2.3','61.1.2.3'),('60.1.2.3','62.1.2.3')],\n",
    "            [('67.1','68.1'),('72.3','73.3'),('76.1.2.3','77.1.2.3'),('76.1.2.3','86.1.2.3'),('97.1.2.3','98.1.2.3')],\n",
    "            [('101.3','102.3'),('105.2','106.2'),('108.1','109.1')]\n",
    "        ]\n",
    "        \n",
    "        self.states = [[0 for _ in range(len(sublist))] for sublist in self.regions_switches]\n",
    "        \n",
    "        self.done=0\n",
    "\n",
    "        \n",
    "    def reset_network(self):\n",
    "        dss.Command('clear')\n",
    "        dss.Text.Command(f'Redirect \"C:/Users/A-F/Desktop/123Bus/IEEE123Master.dss\"')\n",
    "        dss.Solution.Solve()\n",
    "        \n",
    "    def reset_world(self):\n",
    "        self.done = 0\n",
    "        self.states = [[0 for _ in range(len(sublist))] for sublist in self.regions_switches]\n",
    "        self.reset_network()\n",
    "        return self.states\n",
    "            \n",
    "            \n",
    "    def get_reward(self):\n",
    "        rew = 0\n",
    "        x_w = self.get_load_data()\n",
    "        x = self.get_restored_power()\n",
    "\n",
    "        if (x > self.P_gen):\n",
    "            self.done = 1\n",
    "            rew = -10000 * (x - self.P_gen)\n",
    "        else:\n",
    "             rew = x_w\n",
    "        return rew\n",
    "    \n",
    "    \n",
    "    def get_restored_power(self):\n",
    "        dss.Solution.Solve()\n",
    "        powers = []\n",
    "        for load in dss.Loads:\n",
    "            powers.append(dss.CktElement.TotalPowers()[0])\n",
    "            \n",
    "        return sum(powers)\n",
    "        \n",
    "    \n",
    "    def get_status(self, area_idx):\n",
    "        if area_idx >= self.num_areas:\n",
    "            raise ValueError(\"Invalid area index\")\n",
    "        return self.states[area_idx]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def step(self, action_n, action_mask):\n",
    "        obs_n = []\n",
    "        reward_n = []\n",
    "        done_n = []\n",
    "\n",
    "        self.get_connected_pairs_for_regions(action_n, action_mask)\n",
    "        \n",
    "        for area_idx, (action, mask) in enumerate(zip(action_n, action_mask)):\n",
    "            if not mask:\n",
    "                self.step_area(area_idx, action)\n",
    "\n",
    "        \n",
    "        for agent_idx, agent in enumerate(self.agent_n):\n",
    "            obs_n.append(self.get_status(agent_idx))\n",
    "            reward_n.append(self.get_reward())\n",
    "            done_n.append(self.get_done())\n",
    "        \n",
    "        return obs_n, reward_n, done_n\n",
    "\n",
    "    def create_switches(self, bus_pairs):\n",
    "        for pair in bus_pairs:\n",
    "            bus1, bus2 = pair\n",
    "            bus1_first = bus1.split('.')[0]\n",
    "            bus2_first = bus2.split('.')[0]\n",
    "            switch_command = f\"New Line.Sw{bus1_first}{bus2_first} Phases=3 Bus1={bus1} Bus2={bus2} Switch=n  r1=1e-3 r0=1e-3 x1=0.000 x0=0.000 c1=0.000 c0=0.000\"\n",
    "            dss.run_command(switch_command)\n",
    "            dss.Solution.Solve()\n",
    "\n",
    "\n",
    "    def get_connected_pairs(self, switches, binary_list):\n",
    "        connected_pairs = []\n",
    "        for i in range(len(binary_list)):\n",
    "            if binary_list[i] == 1:\n",
    "                connected_pairs.append(switches[i])\n",
    "        return connected_pairs\n",
    "\n",
    "    def get_connected_pairs_for_regions(self, region_switches, action_mask):\n",
    "        region_results = []\n",
    "        for i, (region_switch, mask) in enumerate(zip(region_switches, action_mask)):\n",
    "            if not mask: \n",
    "                result = self.get_connected_pairs(self.regions_switches[i], region_switch)\n",
    "                region_results.append(result)\n",
    "        for i, result in enumerate(region_results):\n",
    "            self.create_switches(result)\n",
    "        return region_results\n",
    " \n",
    "\n",
    "    def get_load_data(self):\n",
    "        dss.Solution.Solve()\n",
    "        powers = []\n",
    "        for load in dss.Loads:\n",
    "            powers.append(dss.CktElement.TotalPowers()[0])\n",
    "\n",
    "\n",
    "        if self.weights is None:\n",
    "            self.weights = [1] * len(powers)\n",
    "        \n",
    "\n",
    "        if len(self.weights) != len(powers):\n",
    "            raise ValueError(\"Length of weights must match the number of power values\")\n",
    "            \n",
    "        weighted_powers = [p * w for p, w in zip(powers, self.weights)]\n",
    "        \n",
    "        return sum(weighted_powers)\n",
    "\n",
    "    def get_done(self):\n",
    "        return self.done\n",
    "    \n",
    "    def step_area(self, area_idx, action):\n",
    "        if area_idx >= self.num_areas:\n",
    "            raise ValueError(\"Invalid area index\")\n",
    "\n",
    "        for switch_idx, act in enumerate(action):\n",
    "            self.states[area_idx][switch_idx] = int(self.states[area_idx][switch_idx]) | int(act)  # Use bitwise OR to update the state\n",
    "\n",
    "world = World()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5858a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agents\n",
    "agent_1 = MADDPG(args, 0)\n",
    "agent_2 = MADDPG(args, 1)\n",
    "agent_3 = MADDPG(args, 2)\n",
    "agent_4 = MADDPG(args, 3)\n",
    "agent_5 = MADDPG(args, 4)\n",
    "\n",
    "agent_n = [agent_1, agent_2, agent_3, agent_4, agent_5]\n",
    "\n",
    "# Initialize parameters\n",
    "max_train_steps = args.max_train_steps\n",
    "evaluate_rewards = []\n",
    "episodes = args.episode_limit\n",
    "\n",
    "replay_buffer = ReplayBuffer(args)\n",
    "total_steps = 0\n",
    "\n",
    "def decay_noise(noise_std, decay_factor, min_noise_std):\n",
    "    noise_std = max(min_noise_std, noise_std * decay_factor)\n",
    "    return noise_std\n",
    "\n",
    "def store_and_train(obs_n, a_n, r_n, obs_next_n, done_n, replay_buffer, agent_n):\n",
    "    replay_buffer.store_transition(obs_n, a_n, r_n, obs_next_n, done_n)\n",
    "    if replay_buffer.current_size > args.batch_size:\n",
    "        for agent_id in range(args.N):\n",
    "            agent_n[agent_id].train(replay_buffer, agent_n)\n",
    "\n",
    "def bitwise_sum_tuples(actions_taken):\n",
    "    bitwise_sum = []\n",
    "    for actions in actions_taken:\n",
    "        if not actions:\n",
    "            continue\n",
    "        tuple_length = len(next(iter(actions)))\n",
    "        result = [0] * tuple_length\n",
    "        for action in actions:\n",
    "            result = [int(a) | int(b) for a, b in zip(result, action)]\n",
    "        bitwise_sum.append(result)\n",
    "    return bitwise_sum\n",
    "\n",
    "def train():\n",
    "    writer = SummaryWriter(log_dir='./runs/experiments')  # Specify your log directory here\n",
    "    noise_std = args.noise_std_init\n",
    "    min_noise_std = args.noise_std_min\n",
    "    decay_factor = 0.85\n",
    "    cumulative_rewards = []\n",
    "    restored_powers = []\n",
    "    moving_avg_window = 10  # You can adjust this window size\n",
    "    moving_avg_rewards = deque(maxlen=moving_avg_window)\n",
    "    moving_avg_restored_powers = deque(maxlen=moving_avg_window)\n",
    "    smoothed_rewards = []\n",
    "    smoothed_restored_powers = []\n",
    "\n",
    "    for total_steps in tqdm(range(args.max_train_steps)):\n",
    "        obs_n = world.reset_world()\n",
    "        actions_taken = [set() for _ in range(args.N)]\n",
    "        cumulative_reward = 0\n",
    "\n",
    "        if total_steps % 1000 == 0:\n",
    "            noise_std = decay_noise(noise_std, decay_factor, min_noise_std)\n",
    "            print(f\"Decaying noise: New noise_std = {noise_std}\")\n",
    "\n",
    "        for episode_step in range(args.episode_limit):\n",
    "            a_n = [agent.choose_action(obs, noise_std) for agent, obs in zip(agent_n, obs_n)]\n",
    "            action_mask = [tuple(a) in actions_taken[i] for i, a in enumerate(a_n)]\n",
    "            obs_next_n, r_n, done_n = world.step(copy.deepcopy(a_n), action_mask)\n",
    "            store_and_train(obs_n, a_n, r_n, obs_next_n, done_n, replay_buffer, agent_n)\n",
    "            obs_n = obs_next_n\n",
    "\n",
    "            for i, a in enumerate(a_n):\n",
    "                actions_taken[i].add(tuple(a))\n",
    "\n",
    "            total_steps += 1\n",
    "            cumulative_reward += r_n[0]\n",
    "\n",
    "            restored_power = world.get_restored_power()\n",
    "\n",
    "            if any(done_n):\n",
    "                print(f\"Episode ended in: {episode_step} steps with cumulative reward of: {cumulative_reward}, and noise std: {noise_std}\")\n",
    "                break\n",
    "\n",
    "        cumulative_rewards.append(cumulative_reward)\n",
    "        restored_powers.append(restored_power)\n",
    "\n",
    "        moving_avg_rewards.append(cumulative_reward)\n",
    "        moving_avg_restored_powers.append(restored_power)\n",
    "\n",
    "        smoothed_reward = np.mean(moving_avg_rewards)\n",
    "        smoothed_restored_power = np.mean(moving_avg_restored_powers)\n",
    "\n",
    "        smoothed_rewards.append(smoothed_reward)\n",
    "        smoothed_restored_powers.append(smoothed_restored_power)\n",
    "\n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar('Cumulative Reward', smoothed_reward, total_steps)\n",
    "        writer.add_scalar('Restored Power', smoothed_restored_power, total_steps)\n",
    "\n",
    "        print(f\"Restored Power: {restored_power}\")\n",
    "        bitwise_sum = bitwise_sum_tuples(actions_taken)\n",
    "        print(f\"Actions taken are: {bitwise_sum}\\n\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "        if total_steps % args.evaluate_freq == 0:\n",
    "            evaluate_policy(total_steps, noise_std)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(smoothed_rewards)\n",
    "    plt.xlabel('step')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Cumulative Reward per step')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.plot(smoothed_restored_powers)\n",
    "    plt.xlabel('step')\n",
    "    plt.ylabel('Restored Power')\n",
    "    plt.title('Restored Power per step')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "def evaluate_policy(total_steps, noise_std):\n",
    "    evaluate_reward = 0\n",
    "    for _ in range(args.evaluate_times):\n",
    "        episode_reward = 0\n",
    "        obs_n = world.reset_world()\n",
    "        actions_taken = [set() for _ in range(args.N)]\n",
    "\n",
    "        for k in range(args.episode_limit):\n",
    "            a_n = [agent.choose_action(obs, 0) for agent, obs in zip(agent_n, obs_n)]\n",
    "            action_mask = [tuple(a) in actions_taken[i] for i, a in enumerate(a_n)]\n",
    "            obs_next_n, r_n, done_n = world.step(copy.deepcopy(a_n), action_mask)\n",
    "            episode_reward += r_n[0]\n",
    "            obs_n = obs_next_n\n",
    "\n",
    "            for i, a in enumerate(a_n):\n",
    "                actions_taken[i].add(tuple(a))\n",
    "\n",
    "            if all(done_n):\n",
    "                break\n",
    "\n",
    "        res_p = world.get_restored_power()\n",
    "        print(f\"Restored power: {res_p}\")\n",
    "        evaluate_reward += episode_reward\n",
    "\n",
    "    evaluate_reward /= args.evaluate_times\n",
    "    evaluate_rewards.append(evaluate_reward)\n",
    "    print(f\"Total steps: {total_steps}\\tEvaluate reward: {evaluate_reward}\\tNoise std: {noise_std}\")\n",
    "\n",
    "# Start training\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a606fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6862e2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tensorboard --logdir=runs --port=1015"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
